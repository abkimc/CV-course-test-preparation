{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNcwmXFpnCEE+Pm21lTOgxS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abkimc/CV-course-test-preparation/blob/main/Image_processing_mock_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hJikfJUavhUZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "db5684a7-cd0f-4a5c-f6ee-1ea57dc3a751"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "\n",
        "# 1.\n",
        "# (a) What is a histogram in image processing? How does it represent pixel intensities?\n",
        "\"\"\"\n",
        "histogram in image processing is a way to display the spread of pixel values over the range of posiible pixel intensities\n",
        "it counts the amount of time each value is used in the image, for example, an image of a tree will see a peek near the green values and a drop near the red values.\n",
        "you can represent each color channel as a histogram, and preform manifulation on them.\n",
        "\"\"\"\n",
        "# (b) Compare histogram stretching and histogram equalization regarding functionality and use cases.\n",
        "\"\"\"\n",
        "histogram stretching - spreading the values of the histogram when the peaks are clutter up and not spread out.\n",
        "can be used to perform color corrections and brighntes adaptioins.\n",
        "histogram equalization - re assigening pixel value in a way that changes the distribuation of a histogram in a way that matches another - can be used to stich images toghter\n",
        "even when there where taken in differnt times.\n",
        "\"\"\"\n",
        "# (c) What is the role of cumulative histograms in image enhancement?\n",
        "\"\"\"\n",
        "They provide a mapping function that redistributes pixel intensities to improve contrast and reveal details in images.\n",
        "The cumulative histogram is used to map the original intensity values to new values. This mapping is done in such a way that the resulting image has a more uniform distribution of intensity levels.\n",
        "\"\"\"\n",
        "\n",
        "# 2.\n",
        "# (a) Explain the concept of a log transformation and its use in image enhancement.\n",
        "\"\"\"\n",
        "log transformation is applying a log transformation with adjusment to the pixel values, by passing each value trough this manipulation, we can get a more clutered histogram wuch add contrast to the image.\n",
        "it squiesih the histogram which usally createa more drakish image.\n",
        "\"\"\"\n",
        "# (b) Describe the contrast stretching process and its impact on image quality.\n",
        "\"\"\"\n",
        "this is basicly streching the histogram to extend in an quite equal matter all over the scale, its a common method to increase the image contrast for images\n",
        "with a narrow range if intensirty values.\n",
        "\"\"\"\n",
        "# (c) Discuss the importance of thresholding in creating binary images.\n",
        "\"\"\"\n",
        "the importance of thresholding in creating binary images is by moving the threshold we change dramticly the valuse we want to keep,\n",
        "this can siginficly affect the image quality, highlight certin elements (great for segmntaion), and drop-insgnificant elements (for noise reduction).\n",
        "\"\"\"\n",
        "\n",
        "# 3.\n",
        "# (a) Describe how a Gaussian filter's standard deviation (œÉ) affects the smoothing level.\n",
        "\"\"\"\n",
        "the higher the standard deviation the stronger the smoothing level, this beacuse the bell shaped is much stiper and dimisih values of pixels stronger.\n",
        "\"\"\"\n",
        "# (b) Explain the relationship between kernel size and œÉ in Gaussian filtering.\n",
        "\"\"\"\n",
        "the bigger the kernel the stronger the effect of the standard deviation,as it sperds out to more pixels\n",
        "\"\"\"\n",
        "# (c) What are the boundary handling techniques for Gaussian filters, and why are they necessary?\n",
        "\"\"\"\n",
        "padding - when the kernel hits the edge of the image. so we can still process those pixels, you can zero pad or use the mean of the surrounding pixels.\n",
        "\"\"\"\n",
        "\n",
        "# 4.\n",
        "# (a) Name and describe three common noise reduction filters.\n",
        "\"\"\"\n",
        "Gaussian blur  - using a kerel the estimates the effect of a gauusinn function of the values of the pixels\n",
        "mean filter  - replaceing the value with the mean of the surrounding pixels\n",
        "median filter - replacing the value with the median of the surrounding pixels\n",
        "\"\"\"\n",
        "# (b) Explain how Gaussian smoothing preserves edges better than simple averaging filters.\n",
        "\"\"\"\n",
        "simple avreging can dranticly change the values when there is a strong intensity changes inside the kernel, edges are just that, thats why the effect of\n",
        "Gaussian smoothing usually reduces them in a less significant way than simple averaging.\n",
        "\"\"\"\n",
        "# (c) Why is noise reduction critical before applying edge detection techniques?\n",
        "\"\"\"\n",
        "edge detection techniques look for strong changes in intensity values the very thing noise reduction decreases, when we don't use noise reduction we risk\n",
        "that a lot of noise will be detected by the edge detection techniques.\n",
        "\"\"\"\n",
        "\n",
        "# 5.\n",
        "# (a) Define feature detection and its significance in image processing.\n",
        "\"\"\"\n",
        "in classical image processing, feature detection reffers to the finding of cerin elemts inside of the image this could be\n",
        "edges, blobs, corner ,points and this helps us to undersand the image\n",
        "\"\"\"\n",
        "# (b) What features can be detected in an image ?\n",
        "\"\"\"\n",
        "edges, corners, blobs, points\n",
        "\"\"\"\n",
        "# (c) Explain the difference between feature detectors and feature descriptors.\n",
        "\"\"\"\n",
        "feature detectors - are used to find feature inside the images, feature descriptors are used to descrive thouse featuers quilites\n",
        "like shape color or other characteristics\n",
        "\"\"\"\n",
        "\n",
        "# 6.\n",
        "# (a) Describe the watershed algorithm and its use in segmenting touching or overlapping objects.\n",
        "\"\"\"\n",
        "the watershed algorithm, first we set markers at minima points in the imagem then we start to \"flood\" the images with colors,\n",
        "this leads to a filing of the image mask, wich in term leads to the boundray identification/\n",
        "\"\"\"\n",
        "# (b) Discuss the challenges of over-segmentation and solutions to mitigate it.\n",
        "\"\"\"\n",
        "beacuse of the overlapping shpaes, we could be in a state of over-segmentation, we can mitagte this by performing noise reduction and  doing some pre procceing to merge similer adjacent regions.\n",
        "\"\"\"\n",
        "# (c) How can markers improve the performance of the watershed algorithm?\n",
        "\"\"\"\n",
        "the markes are the core of this algo, if we do a poor job selecting them, we will get bad results,\n",
        "chousing good markers will lead to better results.\n",
        "\"\"\"\n",
        "\n",
        "# 7.\n",
        "# (a) Explain the principles of the KLT tracker and its reliance on optical flow.\n",
        "\"\"\"\n",
        "The Kanade-Lucas-Tomasi (KLT) tracker finds the movement of a small patch of an image between two frames by minimizing the difference in intensity (brightness) of corresponding pixels.\n",
        "\n",
        "In simple terms, it tries to find the best shift (ùëëùë•,ùëëùë¶) that makes the new patch in the next frame look as similar as possible to the original one.\n",
        "It does this by minimizing the sum of squared differences between pixel values before and after the shift.\n",
        "\n",
        "Once the features are identified, the algorithm uses the Lucas-Kanade optical flow\n",
        "method to estimate their motion between consecutive frames. This approach assumes:\n",
        "Brightness Constancy:\n",
        "‚Ä¢ The intensity of a pixel remains constant between frames.\n",
        "‚Ä¢ Small Motion: The displacement between frames is small and linear.\n",
        "‚Ä¢ Spatial Coherence: Neighboring pixels have similar motions\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "# (b) What are the key steps in selecting \"good features to track\" using the KLT method?\n",
        "\"\"\"\n",
        "Feature Detection: Identify corners or high-gradient regions using methods like Harris corner detection or Shi-Tomasi corner detection.\n",
        "Eigenvalue Analysis: Compute the eigenvalues of the gradient matrix for each feature. Features with large eigenvalues (indicating strong gradients in both x and y directions) are considered \"good.\"\n",
        "Non-Maximum Suppression: Ensure features are well-distributed across the image by suppressing weaker features near stronger ones.\n",
        "Thresholding: Select features that meet a minimum eigenvalue threshold to ensure they are trackable.\n",
        "\"\"\"\n",
        "# (c) Discuss the advantages and limitations of the KLT tracker in handling real-world scenarios.\n",
        "\"\"\"\n",
        "Advantages:\n",
        "\n",
        "Efficiency: Computationally efficient for small displacements and real-time applications.\n",
        "Robustness: Handles partial occlusion and small changes in lighting.\n",
        "Accuracy: Provides precise tracking for features with sufficient texture.\n",
        "\n",
        "Limitations:\n",
        "\n",
        "Large Displacements: Struggles with fast motion or large displacements between frames.\n",
        "Lighting Changes: Performance degrades under significant lighting variations.\n",
        "Occlusions: Fails when features are fully occluded or leave the frame.\n",
        "Texture Dependency: Requires features with sufficient texture; fails in low-texture regions.\n",
        "\"\"\"\n",
        "\n",
        "# 8.\n",
        "# (a) Define Structure from Motion (SfM) and its goals.\n",
        "\"\"\"\n",
        "Structure from Motion (SfM) is a technique used to reconstruct the 3D structure of a scene and estimate camera poses from a sequence of 2D images. Its goals are:\n",
        "\n",
        "3D Reconstruction: Create a sparse or dense 3D model of the scene.\n",
        "Camera Pose Estimation: Determine the position and orientation of the camera for each image.\n",
        "Scene Understanding: Enable applications like 3D mapping, navigation, and augmented reality.\n",
        "\n",
        "\"\"\"\n",
        "# (b) How does SfM differ from stereo vision in terms of assumptions and applications?\n",
        "\"\"\"\n",
        "Differences:\n",
        "\n",
        "Assumptions:\n",
        "SfM: Works with a sequence of images from a single moving camera. It assumes the scene is static or changes minimally.\n",
        "Stereo Vision: Requires two or more cameras with known relative positions. It assumes simultaneous capture of images and known camera geometry.\n",
        "Applications:\n",
        "\n",
        "SfM: Used for 3D reconstruction from monocular video, aerial mapping, and historical photo reconstruction.\n",
        "Stereo Vision: Used in robotics, autonomous vehicles, and real-time depth sensing.\n",
        "\"\"\"\n",
        "# (c) Describe the steps involved in recovering camera poses and 3D structure using SfM.\n",
        "\"\"\"\n",
        "Feature Detection and Matching: Detect keypoints (e.g., using SIFT or ORB) and match them across images.\n",
        "Fundamental Matrix Estimation: Compute the fundamental matrix to establish epipolar geometry between image pairs.\n",
        "Camera Pose Estimation: Recover the relative camera poses (rotation and translation) using techniques like the 8-point algorithm.\n",
        "Triangulation: Compute the 3D positions of matched features using the camera poses.\n",
        "Bundle Adjustment: Refine the 3D structure and camera poses by minimizing reprojection error using nonlinear optimization.\n",
        "\"\"\"\n",
        "# 9.\n",
        "# (a) Explain the triangulation process in 3D stereo imaging.\n",
        "\"\"\"\n",
        "Triangulation is the process of determining the 3D position of a point by using its projections in two or more images taken from different viewpoints. The steps are:\n",
        "Camera Calibration: Know the intrinsic and extrinsic parameters of the cameras.\n",
        "Feature Matching: Identify corresponding points in the stereo images.\n",
        "Epipolar Geometry: Use the camera parameters to compute the intersection of the back-projected rays from the matched points.\n",
        "3D Point Calculation: Solve for the 3D coordinates using the intersection of the rays.\n",
        "\"\"\"\n",
        "# (b) Describe the simplified case of triangulation with aligned cameras.\n",
        "\"\"\"\n",
        "Camera Setup: The cameras have parallel optical axes and are aligned horizontally.\n",
        "Baseline: The distance between the cameras is known.\n",
        "Disparity: The horizontal shift (disparity) of a point in the two images is proportional to its depth.\n",
        "Depth Calculation:\n",
        "Depth Z is computed as:Z = f‚ãÖB / d\n",
        "‚Äã\n",
        "where\n",
        "f is the focal length,\n",
        "B is the baseline, and\n",
        "d is the disparity.\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "# (c) What is the geometric interpretation of the leastsquares approximation in triangulation?\n",
        "\n",
        "\"\"\"\n",
        "The least-squares approximation in triangulation minimizes the error in the 3D position of a point by finding the best intersection of back-projected rays from multiple views. Geometrically:\n",
        "\n",
        "Back-Projected Rays: Each matched point in an image defines a ray in 3D space.\n",
        "\n",
        "Intersection Error: Due to noise, the rays may not intersect perfectly.\n",
        "\n",
        "Least-Squares Solution: The 3D point is computed as the point that minimizes the sum of squared distances to all the rays.\n",
        "\n",
        "This approach provides a robust estimate of the 3D point, especially when more than two views are available.\n",
        "\n",
        "\"\"\"\n",
        "# 10.\n",
        "# (a) Explain the purpose of a confusion matrix in evaluating classification models.\n",
        "\"\"\"\n",
        "A confusion matrix is a table used to evaluate the performance of a classification model by summarizing the predictions compared to the actual labels. It provides a detailed breakdown of:\n",
        "\n",
        "True Positives (TP): Correctly predicted positive instances.\n",
        "True Negatives (TN): Correctly predicted negative instances.\n",
        "False Positives (FP): Negative instances incorrectly predicted as positive.\n",
        "False Negatives (FN): Positive instances incorrectly predicted as negative.\n",
        "\n",
        "The confusion matrix helps calculate metrics like accuracy, precision, recall, and F1-score, providing insights into the model's strengths and weaknesses.\n",
        "\"\"\"\n",
        "# (b) Define and differentiate between precision, recall, and F1-score.\n",
        "\"\"\"\n",
        "Precision: Measures the accuracy of positive predictions.\n",
        "\n",
        "Precision=            True Positives (TP)\n",
        "          True Positives (TP) + False Positives (FP)\n",
        "\n",
        "Focuses on minimizing false positives.\n",
        "Useful when the cost of false positives is high (e.g., spam detection).\n",
        "\n",
        "Recall (Sensitivity): Measures the ability to identify all positive instances.\n",
        "\n",
        "Recall=             True Positives (TP)\n",
        "          True Positives (TP)+False Negatives (FN)\n",
        "\n",
        "Focuses on minimizing false negatives.\n",
        "Useful when the cost of false negatives is high (e.g., medical diagnosis).\n",
        "\n",
        "F1-Score: The harmonic mean of precision and recall, providing a balanced measure.\n",
        "\n",
        "F1-Score= 2 ‚ãÖ Precision ‚ãÖ Recall\n",
        "             Precision + Recall\n",
        "\n",
        "Useful when there is an imbalance between precision and recall.\n",
        "\"\"\"\n",
        "# (c) Define image classification and object recognition.\n",
        "\"\"\"\n",
        "Image Classification: The task of assigning a label to an entire image based on its content. For example, classifying an image as \"cat\" or \"dog.\"\n",
        "Focuses on identifying the dominant object or scene in the image.\n",
        "Typically outputs a single label per image.\n",
        "\n",
        "Object Recognition: The task of identifying and localizing multiple objects within an image. For example, detecting and labeling all cats and dogs in an image.\n",
        "Involves both classification (labeling objects) and localization (drawing bounding boxes around objects).\n",
        "Outputs multiple labels and their locations in the image.\n",
        "\"\"\"\n",
        "# (d) How does object recognition handle multiple objects in an image compared to image classification?\n",
        "\"\"\"\n",
        "Image Classification:\n",
        "\n",
        "Handles only one object or scene per image.\n",
        "Outputs a single label representing the dominant class.\n",
        "Cannot distinguish between multiple objects in the same image.\n",
        "\n",
        "Object Recognition:\n",
        "\n",
        "Handles multiple objects in an image by detecting and localizing each object.\n",
        "Uses techniques like bounding boxes to identify the location of each object.\n",
        "Outputs multiple labels and their corresponding bounding boxes.\n",
        "Examples of object recognition methods include YOLO (You Only Look Once), SSD (Single Shot Detector), and Faster R-CNN.\n",
        "\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SETqbBUrlXwf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}